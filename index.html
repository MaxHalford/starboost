
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Boosting &#8212; StarBoost  documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />
    <script type="text/javascript" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Boosting</a><ul>
<li><a class="reference internal" href="#regression">Regression</a></li>
<li><a class="reference internal" href="#classification">Classification</a></li>
</ul>
</li>
<li><a class="reference internal" href="#losses">Losses</a><ul>
<li><a class="reference internal" href="#l1-loss">L1 loss</a></li>
<li><a class="reference internal" href="#l2-loss">L2 loss</a></li>
<li><a class="reference internal" href="#log-loss">Log loss</a></li>
</ul>
</li>
<li><a class="reference internal" href="#line-searchers">Line searchers</a><ul>
<li><a class="reference internal" href="#line-search-per-leaf">Line search per leaf</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <a class="reference internal image-reference" href="https://docs.google.com/drawings/d/e/2PACX-1vQKggEpm0PGgmkB7LymYmHdptSFEwYXC5yecuph_0gGmZ5fW-bTIfowcDLHVHxjgKQTHq8Y21H0d5LF/pub?w=1277&amp;h=375"><img alt="logo" class="align-center" src="https://docs.google.com/drawings/d/e/2PACX-1vQKggEpm0PGgmkB7LymYmHdptSFEwYXC5yecuph_0gGmZ5fW-bTIfowcDLHVHxjgKQTHq8Y21H0d5LF/pub?w=1277&amp;h=375" style="width: 400px; height: 120px;" /></a>
<div class="toctree-wrapper compound">
</div>
<span class="target" id="module-starboost"></span><p><strong>What is this?</strong></p>
<p>This is the documentation for StarBoost, a Python library that implements gradient boosting. Gradient boosting is an efficient and popular machine learning algorithm used for supervised learning.</p>
<p><strong>Doesn’t scikit-learn already do that?</strong></p>
<p>Indeed scikit-learn <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">implements gradient boosting</a>, but the only supported weak learner is decision tree. In essence gradient boosting can be used with other weak learners than decision trees.</p>
<p><strong>What about XGBoost/LightGBM/CatBoost?</strong></p>
<p>The mentioned libraries are the state of the art of gradient boosting decision trees (GBRT). They implement a specific version of gradient boosting that is tailored to decision trees. StarBoost’s purpose isn’t to compete with them. Instead it’s goal is to implement a generic gradient boosting algorithm that works with any weak learner.</p>
<p>A focus of StarBoost is to keep the code readable and commented, instead of obfuscating the algorithm under a pile of tangled code.</p>
<p><strong>What’s a weak learner?</strong></p>
<p>A weak learner is any machine learning model that can learn from labeled data. It’s called “weak” because it usually works better as part of an ensemble (such as gradient boosting). Examples are linear models, radial basis functions, decision trees, genetic programming, neural networks, etc. In theory you could even use gradient boosting as a weak learner.</p>
<p><strong>Is it compatible with scikit-learn?</strong></p>
<p>Yes, it is.</p>
<p><strong>How do I install it?</strong></p>
<p>Barring any weird Python setup, you simply have to run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">starboost</span></code>.</p>
<p><strong>How do I use it?</strong></p>
<p>The following snippet shows a very basic usage of StarBoost. Please check out the <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">examples directory on GitHub</a> for comprehensive examples.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="kn">import</span> <span class="nn">starboost</span> <span class="kn">as</span> <span class="nn">sb</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_boston</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">sb</span><span class="o">.</span><span class="n">BoostingRegressor</span><span class="p">(</span>
    <span class="n">base_estimator</span><span class="o">=</span><span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>What are you planning on doing next?</strong></p>
<ul class="simple">
<li>Logging the progress</li>
<li>Handling sample weights</li>
<li>Implement more loss functions</li>
<li>Make it faster</li>
<li>Newton boosting (taking into account the information from the Hessian)</li>
<li>Learning to rank</li>
</ul>
<p><strong>By the way, why is it called “StarBoost”?</strong></p>
<p>As you might already know, in programming the star symbol <code class="docutils literal notranslate"><span class="pre">*</span></code> often refers to the concept of “everything”. The idea is that StarBoost can be used with any weak learner, not just decision trees.</p>
<div class="section" id="boosting">
<span id="api"></span><h1>Boosting<a class="headerlink" href="#boosting" title="Permalink to this headline">¶</a></h1>
<div class="section" id="regression">
<h2>Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="starboost.BoostingRegressor">
<em class="property">class </em><code class="descclassname">starboost.</code><code class="descname">BoostingRegressor</code><span class="sig-paren">(</span><em>loss=None</em>, <em>base_estimator=None</em>, <em>base_estimator_is_tree=False</em>, <em>n_estimators=30</em>, <em>init_estimator=None</em>, <em>line_searcher=None</em>, <em>learning_rate=0.1</em>, <em>row_sampling=1.0</em>, <em>col_sampling=1.0</em>, <em>eval_metric=None</em>, <em>early_stopping_rounds=None</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/boosting.html#BoostingRegressor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.BoostingRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradient boosting for regression.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>loss</strong> (<em>starboost.losses.Loss</em><em>, </em><em>default=starboost.loss.L2Loss</em>) – The loss function that will be optimized. At every stage a weak learner will be fit to
the negative gradient of the loss. The provided value must be a class that at the very
least implements a <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method and a <code class="docutils literal notranslate"><span class="pre">gradient</span></code> method.</li>
<li><strong>base_estimator</strong> (<em>sklearn.base.RegressorMixin</em><em>, </em><em>default=None</em>) – The weak learner. This must be
a regression model. If <cite>None</cite> then a decision stump will be used.</li>
<li><strong>base_estimator_is_tree</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Indicates if the provided <code class="docutils literal notranslate"><span class="pre">base_estimator</span></code>
is a tree model or not. Various boosting optimizations specific to trees can be made to
improve the overall performance.</li>
<li><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=30</em>) – The maximum number of weak learners to train. The final
number of trained weak learners will be lower than <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> if early stopping
happens.</li>
<li><strong>init_estimator</strong> (<em>sklearn.base.BaseEstimator</em><em>, </em><em>default=None</em>) – The estimator used to make the
initial guess. If <code class="docutils literal notranslate"><span class="pre">None</span></code> then the <code class="docutils literal notranslate"><span class="pre">init_estimator</span></code> property from the <code class="docutils literal notranslate"><span class="pre">loss</span></code> will
be used.</li>
<li><strong>line_searcher</strong> (<em>starboost.line_searchers.LineSearcher</em><em>, </em><em>default=None</em>) – A line searcher which
can be used to find the optimal step size during gradient descent. If you’ve set
<code class="docutils literal notranslate"><span class="pre">base_estimator_is_tree</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> and are using one of StarBoost’s losses then an optimal
line searcher will be used, meaning you safely set this field to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</li>
<li><strong>learning_rate</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – The learning rate shrinks the contribution of each tree.
Specifically the descent direction estimated by each weak learner will be multiplied by
<code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>. There is a trade-off between learning_rate and <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>.</li>
<li><strong>row_sampling</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – The ratio of rows to sample at each stage.</li>
<li><strong>col_sampling</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – The ratio of columns to sample at each stage.</li>
<li><strong>eval_metric</strong> (<em>function</em><em>, </em><em>default=None</em>) – The evaluation metric used to check for early
stopping. If <code class="docutils literal notranslate"><span class="pre">None</span></code> it will default to <code class="docutils literal notranslate"><span class="pre">loss</span></code>.</li>
<li><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – If int, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> is
the seed used by the random number generator; if <code class="docutils literal notranslate"><span class="pre">RandomState</span></code> instance,
<code class="docutils literal notranslate"><span class="pre">random_state</span></code> is the random number generator; if <code class="docutils literal notranslate"><span class="pre">None</span></code>, the random number
generator is the <code class="docutils literal notranslate"><span class="pre">RandomState</span></code> instance used by <code class="docutils literal notranslate"><span class="pre">np.random</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="starboost.BoostingRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>eval_set=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/boosting.html#BoostingRegressor.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.BoostingRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit a gradient boosting procedure to a dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array-like</em><em> or </em><em>sparse matrix of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input
samples. Sparse matrices are accepted only if they are supported by the weak model.</li>
<li><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – The training target values (strings or integers
in classification, real numbers in regression).</li>
<li><strong>eval_set</strong> (<em>tuple of length 2</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – The evaluation set is a tuple
<code class="docutils literal notranslate"><span class="pre">(X_val,</span> <span class="pre">y_val)</span></code>. It has to respect the same conventions as <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">self</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="starboost.BoostingRegressor.iter_predict">
<code class="descname">iter_predict</code><span class="sig-paren">(</span><em>X</em>, <em>include_init=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/boosting.html#BoostingRegressor.iter_predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.BoostingRegressor.iter_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the predictions for <code class="docutils literal notranslate"><span class="pre">X</span></code> at every stage of the boosting procedure.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array-like</em><em> or </em><em>sparse matrix of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em>) – The input samples.
Sparse matrices are accepted only if they are supported by the weak model.</li>
<li><strong>include_init</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> then the prediction from
<code class="docutils literal notranslate"><span class="pre">init_estimator</span></code> will also be returned.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">iterator of arrays of shape (n_samples,) containing the predicted values at each stage</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="starboost.BoostingRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#starboost.BoostingRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the predictions for <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<p>Under the hood this method simply goes through the outputs of <code class="docutils literal notranslate"><span class="pre">iter_predict</span></code> and returns
the final one.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array-like</em><em> or </em><em>sparse matrix of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.
Sparse matrices are accepted only if they are supported by the weak model.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">array of shape (n_samples,) containing the predicted values.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="starboost.BoostingClassifier">
<em class="property">class </em><code class="descclassname">starboost.</code><code class="descname">BoostingClassifier</code><span class="sig-paren">(</span><em>loss=None</em>, <em>base_estimator=None</em>, <em>base_estimator_is_tree=False</em>, <em>n_estimators=30</em>, <em>init_estimator=None</em>, <em>line_searcher=None</em>, <em>learning_rate=0.1</em>, <em>row_sampling=1.0</em>, <em>col_sampling=1.0</em>, <em>eval_metric=None</em>, <em>early_stopping_rounds=None</em>, <em>random_state=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/boosting.html#BoostingClassifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.BoostingClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Gradient boosting for regression.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>loss</strong> (<em>starboost.losses.Loss</em><em>, </em><em>default=starboost.loss.L2Loss</em>) – The loss function that will be optimized. At every stage a weak learner will be fit to
the negative gradient of the loss. The provided value must be a class that at the very
least implements a <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method and a <code class="docutils literal notranslate"><span class="pre">gradient</span></code> method.</li>
<li><strong>base_estimator</strong> (<em>sklearn.base.RegressorMixin</em><em>, </em><em>default=None</em>) – The weak learner. This must be
a regression model, even though the task is classification. If <cite>None</cite> then a decision
stump will be used.</li>
<li><strong>base_estimator_is_tree</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Indicates if the provided <code class="docutils literal notranslate"><span class="pre">base_estimator</span></code>
is a tree model or not. Various boosting optimizations specific to trees can be made to
improve the overall performance.</li>
<li><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=30</em>) – The maximum number of weak learners to train. The final
number of trained weak learners will be lower than <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> if early stopping
happens.</li>
<li><strong>init_estimator</strong> (<em>sklearn.base.BaseEstimator</em><em>, </em><em>default=None</em>) – The estimator used to make the
initial guess. If <code class="docutils literal notranslate"><span class="pre">None</span></code> then the <code class="docutils literal notranslate"><span class="pre">init_estimator</span></code> property from the <code class="docutils literal notranslate"><span class="pre">loss</span></code> will
be used.</li>
<li><strong>line_searcher</strong> (<em>starboost.line_searchers.LineSearcher</em><em>, </em><em>default=None</em>) – A line searcher which
can be used to find the optimal step size during gradient descent. If you’ve set
<code class="docutils literal notranslate"><span class="pre">base_estimator_is_tree</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> and are using one of StarBoost’s losses then an optimal
line searcher will be used, meaning you safely set this field to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</li>
<li><strong>learning_rate</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – The learning rate shrinks the contribution of each tree.
Specifically the descent direction estimated by each weak learner will be multiplied by
<code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>. There is a trade-off between learning_rate and <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>.</li>
<li><strong>row_sampling</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – The ratio of rows to sample at each stage.</li>
<li><strong>col_sampling</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – The ratio of columns to sample at each stage.</li>
<li><strong>eval_metric</strong> (<em>function</em><em>, </em><em>default=None</em>) – The evaluation metric used to check for early
stopping. If <code class="docutils literal notranslate"><span class="pre">None</span></code> it will default to <code class="docutils literal notranslate"><span class="pre">loss</span></code>.</li>
<li><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – If int, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> is
the seed used by the random number generator; if <code class="docutils literal notranslate"><span class="pre">RandomState</span></code> instance,
<code class="docutils literal notranslate"><span class="pre">random_state</span></code> is the random number generator; if <code class="docutils literal notranslate"><span class="pre">None</span></code>, the random number
generator is the <code class="docutils literal notranslate"><span class="pre">RandomState</span></code> instance used by <code class="docutils literal notranslate"><span class="pre">np.random</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="starboost.BoostingClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>y</em>, <em>eval_set=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/boosting.html#BoostingClassifier.fit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.BoostingClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit a gradient boosting procedure to a dataset.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array-like</em><em> or </em><em>sparse matrix of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input
samples. Sparse matrices are accepted only if they are supported by the weak model.</li>
<li><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – The training target values (strings or integers
in classification, real numbers in regression).</li>
<li><strong>eval_set</strong> (<em>tuple of length 2</em><em>, </em><em>optional</em><em>, </em><em>default=None</em>) – The evaluation set is a tuple
<code class="docutils literal notranslate"><span class="pre">(X_val,</span> <span class="pre">y_val)</span></code>. It has to respect the same conventions as <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">self</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="starboost.BoostingClassifier.iter_predict">
<code class="descname">iter_predict</code><span class="sig-paren">(</span><em>X</em>, <em>include_init=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/boosting.html#BoostingClassifier.iter_predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.BoostingClassifier.iter_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the predicted classes for <code class="docutils literal notranslate"><span class="pre">X</span></code> at every stage of the boosting procedure.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array-like</em><em> or </em><em>sparse matrix of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.
Sparse matrices are accepted only if they are supported by the weak model.</li>
<li><strong>include_init</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> then the prediction from
<code class="docutils literal notranslate"><span class="pre">init_estimator</span></code> will also be returned.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">iterator of arrays of shape (n_samples, n_classes) containing the predicted classes at
each stage.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="starboost.BoostingClassifier.iter_predict_proba">
<code class="descname">iter_predict_proba</code><span class="sig-paren">(</span><em>X</em>, <em>include_init=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/boosting.html#BoostingClassifier.iter_predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.BoostingClassifier.iter_predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the predicted probabilities for <code class="docutils literal notranslate"><span class="pre">X</span></code> at every stage of the boosting procedure.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array-like</em><em> or </em><em>sparse matrix of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.
Sparse matrices are accepted only if they are supported by the weak model.</li>
<li><strong>include_init</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> then the prediction from
<code class="docutils literal notranslate"><span class="pre">init_estimator</span></code> will also be returned.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">iterator of arrays of shape (n_samples, n_classes) containing the predicted
probabilities at each stage</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="starboost.BoostingClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#starboost.BoostingClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the predictions for <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<p>Under the hood this method simply goes through the outputs of <code class="docutils literal notranslate"><span class="pre">iter_predict</span></code> and returns
the final one.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array-like</em><em> or </em><em>sparse matrix of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.
Sparse matrices are accepted only if they are supported by the weak model.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">array of shape (n_samples,) containing the predicted values.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="starboost.BoostingClassifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/boosting.html#BoostingClassifier.predict_proba"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.BoostingClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the predicted probabilities for <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array-like</em><em> or </em><em>sparse matrix of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.
Sparse matrices are accepted only if they are supported by the weak model.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">array of shape (n_samples, n_classes) containing the predicted probabilities.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="losses">
<h1>Losses<a class="headerlink" href="#losses" title="Permalink to this headline">¶</a></h1>
<div class="section" id="l1-loss">
<h2>L1 loss<a class="headerlink" href="#l1-loss" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="starboost.losses.L1Loss">
<em class="property">class </em><code class="descclassname">starboost.losses.</code><code class="descname">L1Loss</code><a class="reference internal" href="_modules/starboost/losses.html#L1Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.losses.L1Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the L1 loss, also known as the mean absolute error.</p>
<p>Mathematically, the L1 loss is defined as</p>
<p><span class="math notranslate nohighlight">\(L = \frac{1}{n} \sum_i^n |p_i - y_i|\)</span></p>
<p>It’s gradient is</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial L}{\partial y_i} = sign(p_i - y_i)\)</span></p>
<p>where <span class="math notranslate nohighlight">\(sign(p_i - y_i)\)</span> is equal to 0 if <span class="math notranslate nohighlight">\(p_i\)</span> is equal to <span class="math notranslate nohighlight">\(y_i\)</span>. Note that
this is slightly different from scikit-learn, which replaces 0s by -1s.</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code> produces mostly the same results as when setting the <code class="docutils literal notranslate"><span class="pre">loss</span></code> parameter to
<code class="docutils literal notranslate"><span class="pre">'lad'</span></code> in scikit-learn’s <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code>.</p>
<dl class="method">
<dt id="starboost.losses.L1Loss.__call__">
<code class="descname">__call__</code><span class="sig-paren">(</span><em>y_true</em>, <em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/losses.html#L1Loss.__call__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.losses.L1Loss.__call__" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the L1 loss.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">starboost</span> <span class="k">as</span> <span class="nn">sb</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sb</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="starboost.losses.L1Loss.default_init_estimator">
<code class="descname">default_init_estimator</code><a class="headerlink" href="#starboost.losses.L1Loss.default_init_estimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">starboost.init.QuantileEstimator(alpha=0.5)</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="starboost.losses.L1Loss.gradient">
<code class="descname">gradient</code><span class="sig-paren">(</span><em>y_true</em>, <em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/losses.html#L1Loss.gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.losses.L1Loss.gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the gradient of the L1 loss with respect to each prediction.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">starboost</span> <span class="k">as</span> <span class="nn">sb</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sb</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">L1Loss</span><span class="p">()</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">array([ 1.,  0., -1.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="starboost.losses.L1Loss.tree_line_searcher">
<code class="descname">tree_line_searcher</code><a class="headerlink" href="#starboost.losses.L1Loss.tree_line_searcher" title="Permalink to this definition">¶</a></dt>
<dd><p>When using <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code> the gradient descent procedure will chase the negative of
<code class="docutils literal notranslate"><span class="pre">L1Loss</span></code>’s gradient. The negative of the gradient is solely composed of 1s, -1s, and 0s.
It turns out that replacing the estimated descent direction with the median of the
according residuals will in fact minimize the overall mean absolute error much faster.</p>
<p>This is exactly the same procedure scikit-learn uses to modify the leaves of decision trees
in <code class="docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code>. However this procedure is more generic and works with any
kind of weak learner.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="l2-loss">
<h2>L2 loss<a class="headerlink" href="#l2-loss" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="starboost.losses.L2Loss">
<em class="property">class </em><code class="descclassname">starboost.losses.</code><code class="descname">L2Loss</code><a class="reference internal" href="_modules/starboost/losses.html#L2Loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.losses.L2Loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the L2 loss, also known as the mean squared error.</p>
<p>Mathematically, the L2 loss is defined as</p>
<p><span class="math notranslate nohighlight">\(L = \frac{1}{n} \sum_i^n (p_i - y_i)^2\)</span></p>
<p>It’s gradient is</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial L}{\partial y_i} = p_i\)</span></p>
<p>Using <cite>MSE</cite> is equivalent to setting the <cite>loss</cite> parameter to <cite>ls</cite> in scikit-learn’s
<cite>GradientBoostingRegressor.</cite></p>
<dl class="method">
<dt id="starboost.losses.L2Loss.__call__">
<code class="descname">__call__</code><span class="sig-paren">(</span><em>y_true</em>, <em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/losses.html#L2Loss.__call__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.losses.L2Loss.__call__" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the L2 loss.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">starboost</span> <span class="k">as</span> <span class="nn">sb</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sb</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">L2Loss</span><span class="p">()(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">25.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="starboost.losses.L2Loss.default_init_estimator">
<code class="descname">default_init_estimator</code><a class="headerlink" href="#starboost.losses.L2Loss.default_init_estimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">starboost.init.MeanEstimator()</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="starboost.losses.L2Loss.gradient">
<code class="descname">gradient</code><span class="sig-paren">(</span><em>y_true</em>, <em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/losses.html#L2Loss.gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.losses.L2Loss.gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the gradient of the L2 loss with respect to each prediction.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">starboost</span> <span class="k">as</span> <span class="nn">sb</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sb</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">L2Loss</span><span class="p">()</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">array([-5,  5,  5])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="log-loss">
<h2>Log loss<a class="headerlink" href="#log-loss" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="starboost.losses.LogLoss">
<em class="property">class </em><code class="descclassname">starboost.losses.</code><code class="descname">LogLoss</code><a class="reference internal" href="_modules/starboost/losses.html#LogLoss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.losses.LogLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the logarithmic loss.</p>
<p>Mathematically, the log loss is defined as</p>
<p><span class="math notranslate nohighlight">\(L = -\frac{1}{n} \sum_i^n y_i log(p_i) + (1-y_i) log(1-p_i)\)</span></p>
<p>It’s gradient is</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial L}{\partial y_i} = sign(p_i - y_i)\)</span></p>
<p>This loss works for binary classification as well as for multi-class cases (in which case the
loss is usually referred to as “cross-entropy”).</p>
<dl class="method">
<dt id="starboost.losses.LogLoss.__call__">
<code class="descname">__call__</code><span class="sig-paren">(</span><em>y_true</em>, <em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/losses.html#LogLoss.__call__"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.losses.LogLoss.__call__" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the log loss.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">starboost</span> <span class="k">as</span> <span class="nn">sb</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sb</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">LogLoss</span><span class="p">()(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.807410...</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="starboost.losses.LogLoss.default_init_estimator">
<code class="descname">default_init_estimator</code><a class="headerlink" href="#starboost.losses.LogLoss.default_init_estimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns <code class="docutils literal notranslate"><span class="pre">starboost.init.PriorProbabilityEstimator()</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="starboost.losses.LogLoss.gradient">
<code class="descname">gradient</code><span class="sig-paren">(</span><em>y_true</em>, <em>y_pred</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/losses.html#LogLoss.gradient"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.losses.LogLoss.gradient" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the gradient of the log loss with respect to each prediction.</p>
<p class="rubric">Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">starboost</span> <span class="k">as</span> <span class="nn">sb</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sb</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">LogLoss</span><span class="p">()</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">array([ 0.5,  0.5, -0.5])</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="line-searchers">
<h1>Line searchers<a class="headerlink" href="#line-searchers" title="Permalink to this headline">¶</a></h1>
<p>During gradient descent the negative gradient of the loss function indicates the direction of descent. A line searcher can be used to determine how far to pursue the direction, or in other words the step size.</p>
<div class="section" id="line-search-per-leaf">
<h2>Line search per leaf<a class="headerlink" href="#line-search-per-leaf" title="Permalink to this headline">¶</a></h2>
<p>One of the reasons why gradient boosting is often used with decision trees is that optimal step sizes exist and are easy to compute.</p>
<dl class="class">
<dt id="starboost.line_searchers.LeafLineSearcher">
<em class="property">class </em><code class="descclassname">starboost.line_searchers.</code><code class="descname">LeafLineSearcher</code><span class="sig-paren">(</span><em>update_leaf</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/starboost/line_searchers.html#LeafLineSearcher"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#starboost.line_searchers.LeafLineSearcher" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="starboost.line_searchers.LeafLineSearcher.__call__">
<code class="descname">__call__</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#starboost.line_searchers.LeafLineSearcher.__call__" title="Permalink to this definition">¶</a></dt>
<dd><p>Call self as a function.</p>
</dd></dl>

</dd></dl>

</div>
</div>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;2018, Max Halford.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.7.4</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    
    <a href="https://github.com/MaxHalford/starboost" class="github">
        <img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"  class="github"/>
    </a>
    

    
  </body>
</html>